{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a21a44",
   "metadata": {},
   "source": [
    "# CART 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103bdf0f",
   "metadata": {},
   "source": [
    "## CART (Classification and Regression Tree)알고리즘 이란?\n",
    "\n",
    "\n",
    "지니계수를 이용해서 Tree알고리즘을 구현한 방식입니다.\n",
    "\n",
    "CART 알고리즘의 특징\n",
    "\n",
    "1. 자식 노드를 두개만 갖는다.\n",
    "2. Regression을 구현 할 수 있다. (불순도를 사용하지 않고 오차를 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be6741",
   "metadata": {},
   "source": [
    "## 지니계수란 ?\n",
    "\n",
    "지니계수는 불순도를 나타내는 지표 중 하나.\n",
    "\n",
    "<img src=\"http://www.learnbymarketing.com/wp-content/uploads/2016/02/gini-index-formula.png\">\n",
    "\n",
    "각 확률간 괴리가 커질 수 록 지니계수가 낮아지는 추세를 보임.\n",
    "\n",
    "예) 이산형 자료에서의 지니계수\n",
    "\n",
    "Case 1) 가슴통증이 있을때(YES) 심장질환이 있다(YES)/없다(NO) 105/39\n",
    "\n",
    "Case 2) 가슴통증이 없을때(NO) 심장질환이 있다(YES)/없다(NO) 39/125\n",
    "\n",
    "이때 두 케이스 지니계수는 1에서 YES의 비율의 제곱과 NO의 비율의 제곱을 뺀것이 됩니다.\n",
    "          \n",
    "따라서, \n",
    "\n",
    "Case 1 의 지니계수는 1 - (105/144)^2 - (39/144)^2 = 0.395\n",
    "\n",
    "Case 2 의 지니계수는 1 - (39/164)^2 - (125/164)^2 = 0.336"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a601f",
   "metadata": {},
   "source": [
    "# ID3 알고리즘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015283e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T13:06:17.396297Z",
     "start_time": "2021-12-05T13:06:17.391297Z"
    }
   },
   "source": [
    "## ID3 알고리즘이란?\n",
    "\n",
    "Iterative Dichotomiser 3의 약자.\n",
    "\n",
    "불순도를 \"엔트로피\"를 이용해서 구하는 알고리즘\n",
    "\n",
    "1. Regression을 구현할수 없다\n",
    "2. 자식노드를 2개이상 분기 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdad05b",
   "metadata": {},
   "source": [
    "## 엔트로피란?\n",
    "\n",
    "<img src =\"https://miro.medium.com/max/1122/0*DkWdyGidNSfdT1Nu.png\">\n",
    "\n",
    "\n",
    "엔트로피 = 확률변수의 놀라움을 측정한 값\n",
    "\n",
    "의사결정 나무에서 엔트로피는\n",
    "\n",
    "분기전 엔트로피(H(S)) - 분기후 엔트로피(H(S,A))로 나타낼 수 있고 이를 Information gain이라고 하며\n",
    "\n",
    "분기 후 엔트로피는 양쪽 가지로 나눠지는 확률 곱하기 엔트로피의 합으로 구할 수 있습니다.\n",
    "\n",
    "최적의 의사결정 나무를 만들기 위해선 분기전 엔트로피(H(S)) - 분기후 엔트로피(H(S,A))의 값이 \"가장 큰\" 변수로 분기합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ec916",
   "metadata": {},
   "source": [
    "# C4.5 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ffba25",
   "metadata": {},
   "source": [
    "## C4.5 알고리즘 이란?\n",
    "\n",
    "ID3 알고리즘을 개선한 알고리즘\n",
    "\n",
    "1. 정교한 불순도 지표 (Information gain ratio) 활용 -> IV(Intrinsic value) 를 사용하여 개선\n",
    "    \n",
    "   Information gain ratio  = IG/IV\n",
    "    \n",
    "\n",
    "2. 범주형 변수 뿐 아니라 연속형 변수를 사용 가능\n",
    "\n",
    "   특정 Threshold를 분기점으로 사용\n",
    "   \n",
    "3. 결측치가 포함된 데이터도 사용 가능\n",
    "\n",
    "   결측치를 뺀 값들로만 엔트로피를 계산\n",
    "   \n",
    "   information gain에 전체 데이터 중 결측치가 아닌 값의 비율을 곱하여 계산\n",
    "   \n",
    "   Intricsic value는 결측치를 하나의 클래스로 보고 포함해서 계산\n",
    "\n",
    "4. 과적합을 방지하기 위한 가지치기\n",
    "\n",
    "   분기가 된 이후 가지치기를 진행하는 방식\n",
    "   \n",
    "   1) 데이터를 train/test  =>  train/pruning/test 데이터로 나눔\n",
    "   \n",
    "   2) training 데이터로 의사결정 나무 생성\n",
    "   \n",
    "   3) pruning data와 test data로 가지치기를 수행\n",
    "   \n",
    "   4) 분기된 가지의 에러가가 부모노드의 에러보다 클 경우 분기를 진행하지 않고 가지치기   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc27a7",
   "metadata": {},
   "source": [
    "# 자료형 별 Desicion Tree의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef349ca",
   "metadata": {},
   "source": [
    "## Desicision Tree 의 학습과정 (이산형 자료)\n",
    "\n",
    "1.  루트 노드가 될 피쳐를 선택한다.\n",
    "\n",
    "피쳐를 통해서 분기된 두 자식노드의 지니계수의 가중평균이 가장 낮은 피쳐가 루트노드로 선택 됨\n",
    "\n",
    "예) 가슴통증에 대한 지니계수의 가중평균은 (144/308)*0.395 + (164/308)*0.336 = 약 0.36\n",
    "\n",
    "2. 자식 노드들을 이용해서 지니계수가 작아지는 방향으로 데이터를 분리해 나간다.\n",
    "\n",
    "    <과정>\n",
    "    \n",
    "    1. 루트노드에서 분기해 나간 자식노드에서도 마찬가지로 지니계수가 가장 적은 피쳐를 이용해서 분기한다.\n",
    "    2. 이 과정을 반복해 나간다.\n",
    "    3. 만약 부모 노드가 자식노드 보다 지니계수가 적다면 분기하지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57ff09",
   "metadata": {},
   "source": [
    "##  Desicision Tree 의 학습과정 (연속형 자료)\n",
    "\n",
    "\n",
    "1. 연속형 자료를 sort한다.\n",
    "2. sort된 자료와 인접한 자료간의 평균을 구한다. 예) 0번 자료와 1번자료의 평균\n",
    "3. 평균을 가지고 평균보다 작은 데이터들이 가지는 타겟값을 이용해서 가장 작은 지니계수를 구해나간다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50469a50",
   "metadata": {},
   "source": [
    "## Desicision Tree 의 학습과정 (순위형 자료)\n",
    "\n",
    "\n",
    "1. 순위보다 작거나 같은 것을 기준으로 지니계수를 구한다.\n",
    "2. 가장 높은 순위는 모든 자료를 포함하기 때문에 계산하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b9925",
   "metadata": {},
   "source": [
    "## Desicision Tree 의 학습과정 (범주형 자료)\n",
    "\n",
    "\n",
    "1. 특정 범주를 골랐을때를 기준으로 지니계수를 구한다.\n",
    "2. 이떄 범주 or 범주를 골랐을때도 계산한다.\n",
    "3. 하지만 모든 범주를 골랐을경우는 모든 자료를 포함하기 때문에 계산하지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920f8c7",
   "metadata": {},
   "source": [
    "# Sklearn 라이브러리의 random forest의 하이퍼 파라미터를 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636956bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "n_estimators : int, default=100\n",
    "\n",
    "    1) 랜덤 포레스트가 갖는 트리의 갯수\n",
    "\n",
    "criterion : {“gini”, “entropy”}, default=”gini”\n",
    "    \n",
    "    1) 불순도를 계산할 방식 'gini' or 'entropy'\n",
    "\n",
    "max_depth : int, default=None\n",
    "\n",
    "    1) 얼마나 깊게 분기할 것인지\n",
    "    \n",
    "    2) None이면 모든 가지가 pure해지거나 min_samples_split 보다 샘플수를 적게 가지게 될때까지 분기 \n",
    "\n",
    "min_samples_split : int or float, default=2\n",
    "\n",
    "    1) 분기하기 위해서 가져야하는 최소한의 sample수\n",
    "    \n",
    "    2) int면 그대로 float형이면 ceil(min_samples_split * n_samples)으로 반영\n",
    "\n",
    "min_samples_leaf : int or float, default=1\n",
    "\n",
    "    1) 자식노드가 되기위해서 필요한 최소 샘플 수\n",
    "\n",
    "min_weight_fraction_leaf : float, default=0.0\n",
    "\n",
    "    1) 자식노드가 되기위해서 필요한 최소 샘플 비율\n",
    "\n",
    "max_features : {“auto”, “sqrt”, “log2”}, int or float, default=”auto”\n",
    "\n",
    "    1) auto, sqrt, log2 중 고를 수 있음\n",
    "    \n",
    "    2) 최선의 분기를 찾기위해 고려해야할 최소한의 피쳐개수\n",
    "    \n",
    "    3) auto, sqrt = sqrt로 계산\n",
    "    \n",
    "    \n",
    "max_leaf_nodes : int, default=None\n",
    "\n",
    "    1) 최대 자식 노드 수\n",
    "    \n",
    "    2) None은 무제한\n",
    "\n",
    "min_impurity_decrease : float, default=0.0\n",
    "\n",
    "    1) 최소 불순도 감소율\n",
    "    \n",
    "bootstrap : bool, default=True\n",
    "    \n",
    "    1) 의사결정 나무를 생성할때 bootstrap할지 여부\n",
    "    \n",
    "oob_score : bool, default=False\n",
    "\n",
    "    1) out of bag 샘플을 이용해서 정규화된 점수를 측정하고 싶을떄\n",
    "    \n",
    "    2) bootstrap이 True일때만 사용가능\n",
    "\n",
    "n_jobs : int, default=None\n",
    "\n",
    "    1) 병렬 작업 몇개? -1은 모든 프로세서 다 사용\n",
    "\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "\n",
    "    1) 랜덤성 컨트롤\n",
    "\n",
    "verbose : int, default=0\n",
    "\n",
    "warm_start : bool, default=False\n",
    "\n",
    "    1) True로 설정하면, 전에 fit 했던 내용을 불러와서 더 많은 estimator를 앙상블에 추가\n",
    "\n",
    "class_weight : {“balanced”, “balanced_subsample”}, dict or list of dicts, default=None\n",
    "\n",
    "    1) {class_label: weight} 같이 딕셔너리 또는 딕셔너리의 리스트를 이용해서 특정 클래스에 가중치를 줄 수 있음\n",
    "    \n",
    "    2) balanced, balanced_subsample 중에 고를 수 있음\n",
    "    \n",
    "    3) balanced를 고르면 입력데이터 중 클래스의 비율에 따라서 가중치를 매김\n",
    "    \n",
    "    4) balanced_subsample은 balanced와 \n",
    "\n",
    "ccp_alpha : non-negative float, default=0.0\n",
    "\n",
    "    1) Minimal Cost-Complexity Pruning을 사용\n",
    "\n",
    "max_samples : int or float, default=None\n",
    "    \n",
    "    1) 각 의사결정트리에 분배할 샘플의 크기\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
